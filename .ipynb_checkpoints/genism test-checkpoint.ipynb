{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "import time\n",
    "# Set file names for train and test data\n",
    "test_data_dir = os.path.join(gensim.__path__[0], 'test', 'test_data')\n",
    "lee_train_file = os.path.join(test_data_dir, 'lee_background.cor')\n",
    "lee_test_file = os.path.join(test_data_dir, 'lee.cor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def remove_values_from_list(the_list, val):\n",
    "   return [value for value in the_list if value != val]\n",
    "\n",
    "def filter(document):\n",
    "        # Filter out components that are not needed \n",
    "    i = 0\n",
    "    df = None\n",
    "    noun_list = []\n",
    "    while i < len(document):\n",
    "        \n",
    "        # Convert list of strings to natural language\n",
    "#         while 'A' in document[i]: document[i].remove('A')\n",
    "#         while 'start' in document[i]: document[i].remove('start')\n",
    "#         while 'end' in document[i]: document[i].remove('end')\n",
    "#         while '>' in document[i]: document[i].remove('>')\n",
    "#         while '<' in document[i]: document[i].remove('<')\n",
    "        noun_str = ''.join(document[i])\n",
    "        noun_list = [*noun_list, noun_str]\n",
    "        \n",
    "        i += 1\n",
    "    return noun_list\n",
    "a = ['skateboarder trick skateboard ramp',\n",
    " 'person air skis',\n",
    " 'wood door boards',\n",
    " 'Do Enter sign road stadium',\n",
    " 'Small child chair plate',\n",
    " 'groups people toilet area',\n",
    " 'hand cellphone',\n",
    " 'People computers student room',\n",
    " 'birds field',\n",
    " 'man cell phone park',\n",
    " 'group men table microphones speech',\n",
    " 'bathroom toilet sprayer wall',\n",
    " 'woman bench phone',\n",
    " 'woman clock purse market',\n",
    " 'surfer hand signal',\n",
    " 'cat asphalt',\n",
    " 'carrots cut squash carrots',\n",
    " 'Traffic lights intersection world',\n",
    " 'man goatee backseat vehicle luggage',\n",
    " 'street sign intersection Beacon Ave Stevens St.',\n",
    " 'kid skateboard kid',\n",
    " 'apple clock display',\n",
    " 'bunch color watches table',\n",
    " 'man suit standing front stove',\n",
    " 'Pizzas sauce cheese table',\n",
    " 'variety vegetables sticks tray control',\n",
    " 'bus road driver',\n",
    " 'pizza cut pieces top counter',\n",
    " 'plate breakfast food eggs toast hash browns',\n",
    " 'transit bus lot',\n",
    " 'man air skateboard',\n",
    " 'ground plane',\n",
    " 'person bananas back',\n",
    " 'Three zebras field grass',\n",
    " 'couple pieces toast cup syrup',\n",
    " 'Many dishes people',\n",
    " 'herd wire fence',\n",
    " 'slice pizza cheese crust',\n",
    " 'herd cows field',\n",
    " 'paper plate dog sandwich cream cheese',\n",
    " 'baby boy room baby doll',\n",
    " 'man woman tennis rackets court',\n",
    " 'bus side man storage bus']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['skateboarder', 'trick', 'skateboard', 'ramp'], tags=[0]), TaggedDocument(words=['person', 'air', 'skis'], tags=[1]), TaggedDocument(words=['wood', 'door', 'boards'], tags=[2]), TaggedDocument(words=['do', 'enter', 'sign', 'road', 'stadium'], tags=[3]), TaggedDocument(words=['small', 'child', 'chair', 'plate'], tags=[4]), TaggedDocument(words=['groups', 'people', 'toilet', 'area'], tags=[5]), TaggedDocument(words=['hand', 'cellphone'], tags=[6]), TaggedDocument(words=['people', 'computers', 'student', 'room'], tags=[7]), TaggedDocument(words=['birds', 'field'], tags=[8]), TaggedDocument(words=['man', 'cell', 'phone', 'park'], tags=[9]), TaggedDocument(words=['group', 'men', 'table', 'microphones', 'speech'], tags=[10]), TaggedDocument(words=['bathroom', 'toilet', 'sprayer', 'wall'], tags=[11]), TaggedDocument(words=['woman', 'bench', 'phone'], tags=[12]), TaggedDocument(words=['woman', 'clock', 'purse', 'market'], tags=[13]), TaggedDocument(words=['surfer', 'hand', 'signal'], tags=[14]), TaggedDocument(words=['cat', 'asphalt'], tags=[15]), TaggedDocument(words=['carrots', 'cut', 'squash', 'carrots'], tags=[16]), TaggedDocument(words=['traffic', 'lights', 'intersection', 'world'], tags=[17]), TaggedDocument(words=['man', 'goatee', 'backseat', 'vehicle', 'luggage'], tags=[18]), TaggedDocument(words=['street', 'sign', 'intersection', 'beacon', 'ave', 'stevens', 'st'], tags=[19]), TaggedDocument(words=['kid', 'skateboard', 'kid'], tags=[20]), TaggedDocument(words=['apple', 'clock', 'display'], tags=[21]), TaggedDocument(words=['bunch', 'color', 'watches', 'table'], tags=[22]), TaggedDocument(words=['man', 'suit', 'standing', 'front', 'stove'], tags=[23]), TaggedDocument(words=['pizzas', 'sauce', 'cheese', 'table'], tags=[24]), TaggedDocument(words=['variety', 'vegetables', 'sticks', 'tray', 'control'], tags=[25]), TaggedDocument(words=['bus', 'road', 'driver'], tags=[26]), TaggedDocument(words=['pizza', 'cut', 'pieces', 'top', 'counter'], tags=[27]), TaggedDocument(words=['plate', 'breakfast', 'food', 'eggs', 'toast', 'hash', 'browns'], tags=[28]), TaggedDocument(words=['transit', 'bus', 'lot'], tags=[29]), TaggedDocument(words=['man', 'air', 'skateboard'], tags=[30]), TaggedDocument(words=['ground', 'plane'], tags=[31]), TaggedDocument(words=['person', 'bananas', 'back'], tags=[32]), TaggedDocument(words=['three', 'zebras', 'field', 'grass'], tags=[33]), TaggedDocument(words=['couple', 'pieces', 'toast', 'cup', 'syrup'], tags=[34]), TaggedDocument(words=['many', 'dishes', 'people'], tags=[35]), TaggedDocument(words=['herd', 'wire', 'fence'], tags=[36]), TaggedDocument(words=['slice', 'pizza', 'cheese', 'crust'], tags=[37]), TaggedDocument(words=['herd', 'cows', 'field'], tags=[38]), TaggedDocument(words=['paper', 'plate', 'dog', 'sandwich', 'cream', 'cheese'], tags=[39]), TaggedDocument(words=['baby', 'boy', 'room', 'baby', 'doll'], tags=[40]), TaggedDocument(words=['man', 'woman', 'tennis', 'rackets', 'court'], tags=[41]), TaggedDocument(words=['bus', 'side', 'man', 'storage', 'bus'], tags=[42])]\n"
     ]
    }
   ],
   "source": [
    "def read_corpus(f, tokens_only=False):\n",
    "#     print(f)\n",
    "    for i, line in enumerate(f):\n",
    "        tokens = gensim.utils.simple_preprocess(line)\n",
    "        if tokens_only:\n",
    "            yield tokens\n",
    "        else:\n",
    "            # For training data, add tags\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "r = list(read_corpus(filter(a)))\n",
    "a = list(read_corpus(filter(a), tokens_only=True))\n",
    "\n",
    "print(r)\n",
    "# print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "\n",
    "import multiprocessing \n",
    "\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "\n",
    "# w2v_model = Doc2Vec(min_count=20,\n",
    "#                      window=2,\n",
    "#                      size=300,\n",
    "#                      sample=6e-5, \n",
    "#                      alpha=0.03, \n",
    "#                      min_alpha=0.0007, \n",
    "#                      negative=20,\n",
    "#                      workers=cores-1)\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40,  \n",
    "                                      workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.0 mins\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "model.build_vocab(r, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time.time() - t) / 60, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.06 mins\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.train(r, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time.time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.2339795e-03  5.5544190e-03  9.2203671e-04  5.1273340e-03\n",
      "  8.8215759e-03 -6.4809807e-03 -7.6325331e-03 -8.0525271e-05\n",
      " -2.0110351e-03  1.3126152e-03 -5.2263886e-03 -4.3702321e-03\n",
      "  9.6780108e-03 -6.4570140e-03 -1.6096502e-03 -5.3235772e-03\n",
      "  1.7050403e-03 -1.2699522e-03 -7.4769999e-03 -2.7769834e-03\n",
      "  5.8344514e-03  9.3286512e-03 -7.1499320e-03 -9.5871305e-03\n",
      "  3.4184933e-03  5.0515425e-03 -7.6746522e-03 -3.3546770e-03\n",
      " -3.6045696e-04  7.1386131e-03  1.3557952e-03 -1.4396709e-03\n",
      " -7.4540498e-03 -7.7685160e-03 -1.7472230e-03 -8.5792299e-03\n",
      " -2.3522419e-03  3.0952436e-03 -5.5612861e-03  4.5095109e-03\n",
      "  9.2417086e-03 -5.5084899e-03 -6.1000376e-03  9.7689405e-03\n",
      " -6.0386080e-03 -3.1133282e-03 -3.4274673e-03  2.3015474e-04\n",
      "  2.2461531e-03 -3.2637829e-03]\n"
     ]
    }
   ],
   "source": [
    "vector = model.infer_vector(['only', 'you'])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model assesssment ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(r)):\n",
    "    inferred_vector = model.infer_vector(r[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({31: 3, 15: 3, 42: 3, 37: 3, 2: 2, 1: 2, 16: 2, 23: 2, 22: 2, 19: 2, 33: 2, 13: 1, 21: 1, 18: 1, 10: 1, 35: 1, 3: 1, 29: 1, 11: 1, 26: 1, 0: 1, 12: 1, 17: 1, 39: 1, 5: 1, 20: 1, 25: 1, 28: 1})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document (42): «bus side man storage bus»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (12, 0.3003784418106079): «woman bench phone»\n",
      "\n",
      "SECOND-MOST (17, 0.2788783609867096): «traffic lights intersection world»\n",
      "\n",
      "MEDIAN (15, 0.038457997143268585): «cat asphalt»\n",
      "\n",
      "LEAST (11, -0.2805737257003784): «bathroom toilet sprayer wall»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(r[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(r[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Document (15): «cat asphalt»\n",
      "\n",
      "Similar Document (11, 0.28561028838157654): «bathroom toilet sprayer wall»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the corpus and infer a vector from the model\n",
    "import random\n",
    "doc_id = random.randint(0, len(r) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(r[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(r[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Model\n",
    "\n",
    "Using the same approach above, we’ll infer the vector for a randomly chosen test document, and compare the docum\n",
    "ent to our model by eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (25): «TaggedDocument(['variety', 'vegetables', 'sticks', 'tray', 'control'], [25])»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (38, 0.2854100167751312): «herd cows field»\n",
      "\n",
      "MEDIAN (15, -0.02598617784678936): «cat asphalt»\n",
      "\n",
      "LEAST (14, -0.3865185081958771): «surfer hand signal»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, r[doc_id]))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(r[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
