{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OrYzq9vR1v7U"
   },
   "source": [
    "# Instructions and Remarks #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RRKiRaNC04nm"
   },
   "source": [
    "*   Download this Jupyter Notebook and upload it on google drive. Then, open and run via google collab.\n",
    "*   A large dataset will be downloaded for this project. Among the data, you may choose the number of samplongs that you may use when running the notebook.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ln9RLEMT3UKT"
   },
   "source": [
    "# Objective #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FlVLkMHV3Xk1"
   },
   "source": [
    "\n",
    "\n",
    "1.   First, we will collect various samples of the COCO dataset and filter out the nouns for each sample.\n",
    "2.   Then, we will filter out the nouns and compare it with each other, and measure the cosine similarity.\n",
    "3. We will then show a graphical representation of how similar the items are.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qgPxexU515tn"
   },
   "source": [
    "![alt text](https://miro.medium.com/max/442/1*UODvtQMybHE8c0eL3K5z5A.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xl98BwVx31vB"
   },
   "source": [
    "   4.  We will then proceed to use the training samples and predict how well the nouns are predicted, with the information of how \"similar\" the nouns are, which we retrieved from finding the cosine similarity.\n",
    "   5. We will also show a graphical representation of such, to draw conclusio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlbYnNXi3F7x"
   },
   "source": [
    "\n",
    "![alt text](https://miro.medium.com/max/886/1*yiH5sZI-IBxDSQMKhvbcHw.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0bNuN_x-Usd"
   },
   "source": [
    "# Import Statements #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zVrqpNoRQPRa"
   },
   "source": [
    "We will first import the appropriate modules which we will be using right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SSZIjvGG--Us"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UAczlzdR-BGl",
    "outputId": "882e2edb-1b26-465c-9da6-c80fafd33b7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "try:\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "print(\"Imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WMVZW7nL_zJY"
   },
   "source": [
    "# Import Dataset #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M5zIDnqV4x2Q"
   },
   "source": [
    "We will download the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "jS8RtAJMASRT",
    "outputId": "3a2131fc-a193-4af1-8f24-7deaec78a363"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
      "252878848/252872794 [==============================] - 16s 0us/step\n",
      "Downloading data from http://images.cocodataset.org/zips/train2014.zip\n",
      "13510574080/13510573713 [==============================] - 802s 0us/step\n"
     ]
    }
   ],
   "source": [
    "annotation_folder = '/annotations/'\n",
    "if not os.path.exists(os.path.abspath('.') + annotation_folder):\n",
    "  annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
    "                                          cache_subdir=os.path.abspath('.'),\n",
    "                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
    "                                          extract = True)\n",
    "  annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n",
    "  os.remove(annotation_zip)\n",
    "image_folder = '/train2014/'\n",
    "if not os.path.exists(os.path.abspath('.') + image_folder):\n",
    "  image_zip = tf.keras.utils.get_file('train2014.zip',\n",
    "                                      cache_subdir=os.path.abspath('.'),\n",
    "                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
    "                                      extract = True)\n",
    "  PATH = os.path.dirname(image_zip) + image_folder\n",
    "  os.remove(image_zip)\n",
    "else:\n",
    "  PATH = os.path.abspath('.') + image_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kuk1NGil5E4S"
   },
   "source": [
    "We will restrict the training size by setting up a certain limit. This case, we are using 10000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V7TM7N6_5Cps"
   },
   "outputs": [],
   "source": [
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "all_captions = []\n",
    "all_img_name_vector = []\n",
    "\n",
    "for annot in annotations['annotations']:\n",
    "    caption = '<start> ' + annot['caption'] + ' <end>'\n",
    "    image_id = annot['image_id']\n",
    "    full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "    all_img_name_vector.append(full_coco_image_path)\n",
    "    all_captions.append(caption)\n",
    "\n",
    "train_captions, img_name_vector = shuffle(all_captions,\n",
    "                                          all_img_name_vector,\n",
    "                                          random_state=1)\n",
    "num_examples = 100\n",
    "train_captions = train_captions[:num_examples]\n",
    "img_name_vector = img_name_vector[:num_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4bgN7c4JPaC1",
    "outputId": "848b9e27-2e54-4582-b2f4-4ae954b459ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_captions)\n",
    "# type(train_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kiwy8mWJeJnw"
   },
   "source": [
    "# Extracting the **Nouns** from the Restricted Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P9TgCSy7YBLC"
   },
   "source": [
    "Now that we have limited the datatset for training, we will construct an algorithm so that we are able to filter out the nouns.\n",
    "\n",
    "We wil use NLTK library, to filter out the nouns.\n",
    "\n",
    "Incase your NTLK packages are not downloaded, this jupyter notebook will allow you to download the 'punkt' package and the 'averaged_perceptron_tagger' package. \n",
    "\n",
    "If you already have the NLTK libraries, comment out \"nltk.download()\" portion below the import statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5ZXuQytnzaPT",
    "outputId": "175dfe50-8075-4f0a-fe9a-d211ccbb7cbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['start', 'skateboarder', 'trick', 'skateboard', 'ramp', 'end', '>'],\n",
       " ['start', 'person', 'air', 'skis', 'end', '>'],\n",
       " ['start', 'wood', 'door', 'boards', 'end', '>'],\n",
       " ['start', 'A', 'Do', 'Enter', 'sign', 'road', 'stadium', 'end', '>'],\n",
       " ['start', '>', 'Small', 'child', 'chair', 'plate', 'end', '>'],\n",
       " ['start', '>', 'groups', 'people', 'toilet', 'area', 'end', '>'],\n",
       " ['start', 'hand', 'cellphone', 'end', '>'],\n",
       " ['start', '>', 'People', 'computers', 'student', 'room', 'end', '>'],\n",
       " ['start', '>', 'birds', 'field', 'end', '>'],\n",
       " ['start', 'man', 'cell', 'phone', 'park', 'end', '>'],\n",
       " ['start', 'A', 'group', 'men', 'table', 'microphones', 'speech', 'end', '>'],\n",
       " ['start', 'bathroom', 'toilet', 'sprayer', 'wall', 'end', '>'],\n",
       " ['start', 'A', 'woman', 'bench', 'phone', 'end', '>'],\n",
       " ['start', 'woman', 'clock', 'purse', 'market', 'end', '>'],\n",
       " ['start', 'surfer', 'hand', 'signal', 'end', '>'],\n",
       " ['start', 'A', 'cat', 'asphalt', 'end', '>'],\n",
       " ['start', '>', 'carrots', 'cut', 'squash', 'carrots', 'end', '>'],\n",
       " ['start', '>', 'Traffic', 'lights', 'intersection', 'world', 'end', '>'],\n",
       " ['start', 'man', 'goatee', 'backseat', 'vehicle', 'luggage', 'end', '>'],\n",
       " ['start',\n",
       "  'A',\n",
       "  'street',\n",
       "  'sign',\n",
       "  'intersection',\n",
       "  'Beacon',\n",
       "  'Ave',\n",
       "  'Stevens',\n",
       "  'St.',\n",
       "  '<',\n",
       "  'end',\n",
       "  '>'],\n",
       " ['start', 'kid', 'skateboard', 'kid', 'end', '>'],\n",
       " ['start', 'A', 'apple', 'clock', 'display', 'end', '>'],\n",
       " ['start', 'A', 'bunch', 'color', 'watches', 'table', 'end', '>'],\n",
       " ['start', 'man', 'suit', 'standing', 'front', 'stove', 'end', '>'],\n",
       " ['start', '>', 'Pizzas', 'sauce', 'cheese', 'table', 'end', '>'],\n",
       " ['start', 'variety', 'vegetables', 'sticks', 'tray', 'control', 'end', '>'],\n",
       " ['start', 'bus', 'road', 'driver', 'end', '>'],\n",
       " ['start', 'A', 'pizza', 'cut', 'pieces', 'top', 'counter', 'end', '>'],\n",
       " ['start',\n",
       "  'plate',\n",
       "  'breakfast',\n",
       "  'food',\n",
       "  'eggs',\n",
       "  'toast',\n",
       "  'hash',\n",
       "  'browns',\n",
       "  'end',\n",
       "  '>'],\n",
       " ['start', 'transit', 'bus', 'lot', 'end', '>'],\n",
       " ['start', 'man', 'air', 'skateboard', 'end', '>'],\n",
       " ['start', '>', 'ground', 'plane', 'end', '>'],\n",
       " ['start', 'person', 'bananas', 'back', 'end', '>'],\n",
       " ['start', '>', 'Three', 'zebras', 'field', 'grass', 'end', '>'],\n",
       " ['start', 'A', 'couple', 'pieces', 'toast', 'cup', 'syrup', 'end', '>'],\n",
       " ['start', '>', 'Many', 'dishes', 'people', 'end', '>'],\n",
       " ['start', 'herd', 'wire', 'fence', 'end', '>'],\n",
       " ['start', 'A', 'slice', 'pizza', 'cheese', 'crust', 'end', '>'],\n",
       " ['start', 'herd', 'cows', 'field', 'end', '>'],\n",
       " ['start', 'paper', 'plate', 'dog', 'sandwich', 'cream', 'cheese', 'end', '>'],\n",
       " ['start', 'baby', 'boy', 'room', 'baby', 'doll', 'end', '>'],\n",
       " ['start', 'man', 'woman', 'tennis', 'rackets', 'court', 'end', '>'],\n",
       " ['start', 'bus', 'side', 'man', 'storage', 'bus', '<', 'end', '>'],\n",
       " ['start', 'A', 'girl', 'man', 'tie', 'end', '>'],\n",
       " ['start', 'A', 'baseball', 'player', 'bat', 'game', 'end', '>'],\n",
       " ['start', 'man', 'ocean', 'waters', 'waves', 'end', '>'],\n",
       " ['start', 'A', 'couple', 'women', 'top', 'tennis', 'court', 'end', '>'],\n",
       " ['start', '>', 'zebra', 'end', '>'],\n",
       " ['start', 'clock', 'side', 'beige', 'bell', 'tower', 'end', '>'],\n",
       " ['start', 'bear', 'teeth', 'camera', 'end', '>'],\n",
       " ['start', 'A', 'remote', 'sink', 'room', 'end', '>'],\n",
       " ['start', 'A', 'bathroom', 'sink', 'mirror', 'toilet', 'tub', 'end', '>'],\n",
       " ['start', 'passenger', 'train', 'platform', 'end', '>'],\n",
       " ['start', 'A', 'group', 'people', 'restaurant', 'table', 'end', '>'],\n",
       " ['start', 'men', 'glasses', 'end', '>'],\n",
       " ['start', 'A', 'woman', 'slice', 'pizza', 'cheese', 'end', '>'],\n",
       " ['start', 'A', 'motor', 'bike', 'side', 'road', 'end', '>'],\n",
       " ['start', 'plate', 'foods', 'cheeses', 'end', '>'],\n",
       " ['start', '>', 'Donuts', 'cell', 'phone', 'table', 'end', '>'],\n",
       " ['start', '>', 'Six', 'plastic', 'containers', 'vegetables', 'end', '>'],\n",
       " ['start', 'horse', 'standing', 'front', 'portrait', 'village', 'end', '>'],\n",
       " ['start', 'women', 'hill', 'end', '>'],\n",
       " ['start', 'man', 'glasses', 'suit', 'vest', 'end', '>'],\n",
       " ['start',\n",
       "  'kitchen',\n",
       "  'counters',\n",
       "  'chrome',\n",
       "  'microwave',\n",
       "  'backsplash',\n",
       "  'end',\n",
       "  '>'],\n",
       " ['start', 'retriever', 'dog', 'desk', 'end', '>'],\n",
       " ['start', 'A', 'dog', 'leash', 'reflection', 'door', 'end', '>'],\n",
       " ['start', 'giraffe', 'umbrella', 'end', '>'],\n",
       " ['start', 'skier', 'blue', 'jacket', 'goggles', 'end', '>'],\n",
       " ['start', 'skier', 'view', 'mountain', 'top', 'end', '>'],\n",
       " ['start', 'truck', 'highway', 'end', '>'],\n",
       " ['start', 'batter', 'plate', 'swing', 'pitch', 'end', '>'],\n",
       " ['start', 'train', 'towards', 'train', 'station', 'end', '>'],\n",
       " ['start', '>', 'People', 'baggage', 'claim', 'area', 'airport', 'end', '>'],\n",
       " ['start', 'bride', 'groom', 'wedding', 'cake', 'end', '>'],\n",
       " ['start', '>', 'Parking', 'meters', 'front', 'spaces', 'lot', 'end', '>'],\n",
       " ['start', 'A', 'bunch', 'road', 'signs', 'side', 'road', 'end', '>'],\n",
       " ['start', 'plate', 'food', 'table', 'pitchers', 'end', '>'],\n",
       " ['start', 'laptop', 'desk', 'coke', 'bottle', 'Apple', 'monitor', 'end', '>'],\n",
       " ['start', 'A', 'person', 'skis', 'lays', 'snow', 'legs', 'end', '>'],\n",
       " ['start', '>', 'Cars', 'street', 'traffic', 'light', 'end', '>'],\n",
       " ['start', '>', 'people', 'tables', 'outdoors', 'end', '>'],\n",
       " ['start', 'giraffe', 'boulder', 'wall', 'end', '>'],\n",
       " ['start', 'A', 'leather', 'chair', 'book', 'arm', 'end', '>'],\n",
       " ['start', 'A', 'group', 'cross', 'country', 'skiiers', 'race', 'end', '>'],\n",
       " ['start', 'table', 'plates', 'food', 'end', '>'],\n",
       " ['start', 'A', 'couple', 'animals', 'day', 'end', '>'],\n",
       " ['start', 'man', 'tie', 'front', 'mirror', 'end', '>'],\n",
       " ['start', 'cat', 'piece', 'luggage', 'end', '>'],\n",
       " ['start', 'elephant', 'background', 'monkey', 'sides', 'end', '>'],\n",
       " ['start', 'man', 'tennis', 'ball', 'end', '>'],\n",
       " ['start', 'A', 'bathroom', 'door', 'end', '>'],\n",
       " ['start', 'bar', 'blender', 'liquid', 'end', '>'],\n",
       " ['start', 'girafee', 'top', 'tree', 'girafee', 'end', '>'],\n",
       " ['start', 'A', 'clock', 'tower', 'towering', 'city', 'end', '>'],\n",
       " ['start', 'man', 'motor', 'cycle', 'saddle', 'bags', 'end', '>'],\n",
       " ['start', 'mug', 'keyboard', 'desk', 'end', '>'],\n",
       " ['start', 'man', 'drives', 'horse', 'carriage', 'end', '>'],\n",
       " ['start', '>', 'zebras', 'side', 'side', 'grass', 'end', '>'],\n",
       " ['start',\n",
       "  '>',\n",
       "  'Inside',\n",
       "  'apartment',\n",
       "  'door',\n",
       "  'chairs',\n",
       "  'refrigerator',\n",
       "  'end',\n",
       "  '>'],\n",
       " ['start', 'bathroom', 'beige', 'flooring', 'walls', 'end', '>']]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# lines = [['lines is was some string of words'], ['My name is Jeff']]\n",
    "\n",
    "def filter_nouns(data):\n",
    "    i = 0\n",
    "    nouns = None\n",
    "    noun_list = []\n",
    "    while i < len(data):\n",
    "        lines = ''.join(data[i])\n",
    "        # function to test if something is a noun\n",
    "        is_noun = lambda pos: pos[:2] == 'NN'\n",
    "        # do the nlp stuff\n",
    "        tokenized = nltk.word_tokenize(lines)\n",
    "        nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n",
    "        noun_list = [*noun_list, nouns]\n",
    "        i += 1     \n",
    "    return noun_list\n",
    "    \n",
    "filter_nouns(train_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_PBXEQzngGnT"
   },
   "source": [
    "# Observing Scene Prediciton with Cosine and Soft cosine similarity # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a62fK3-IeYmj"
   },
   "source": [
    "## Cosine Similarity Analysis ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ipkcc6xteCh_"
   },
   "source": [
    "*TODO*\n",
    "\n",
    "*The captions are a list of strings, convert to a list of lists and filter our the nouns from that list -- done*\n",
    "\n",
    "*Find the cosine similarity between the lists*\n",
    "\n",
    "*Add a new caption, remove a noun and see if our model can predict the possible noun associated with that catption* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "luzT3BsBBa6R"
   },
   "source": [
    "With the obtained list of lists which contains the nouns for each training set, we will now calculate the cosine similarity and plot a visual representation of how similar each captions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WBi9-BJBEUT3",
    "outputId": "c6dc29ad-0484-470b-b351-b0e08872567e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9449111825230683\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    " \n",
    "# vectors\n",
    "a = np.array([1,2,3])\n",
    "b = np.array([1,1,4])\n",
    " \n",
    "# manually compute cosine similarity\n",
    "dot = np.dot(a, b)\n",
    "norma = np.linalg.norm(a)\n",
    "normb = np.linalg.norm(b)\n",
    "cos = dot / (norma * normb)\n",
    " \n",
    "# use library, operates on sets of vectors\n",
    "aa = a.reshape(1,3)\n",
    "ba = b.reshape(1,3)\n",
    "cos_lib = cosine_similarity(aa, ba)\n",
    " \n",
    "print(\n",
    "\n",
    "    cos_lib[0][0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oiMmEMVjEY_S"
   },
   "source": [
    "We will attempt to find this similarity in case of a bag of words.\n",
    "\n",
    "\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/cosine-similarity/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5iFgMQzJEaEJ"
   },
   "outputs": [],
   "source": [
    "# Define the documents\n",
    "doc_trump = \"Mr. Trump became president after winning the political election. \\\n",
    "Though he lost the support of some republican friends, Trump is friends with President Putin\"\n",
    "\n",
    "doc_election = \"President Trump says Putin had no political interference is the election outcome. \\\n",
    "He says it was a witchhunt by political parties. He claimed President Putin is a friend who had nothing \\\n",
    "to do with the election\"\n",
    "\n",
    "doc_putin = \"Post elections, Vladimir Putin became President of Russia. President Putin had served as \\\n",
    "the Prime Minister earlier in his political career\"\n",
    "\n",
    "documents = [doc_trump, doc_election, doc_putin]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "colab_type": "code",
    "id": "D_ObHSxkEfZr",
    "outputId": "3a0b01e9-a367-4dc1-f6b0-129e5f7ec571"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>after</th>\n",
       "      <th>as</th>\n",
       "      <th>became</th>\n",
       "      <th>by</th>\n",
       "      <th>career</th>\n",
       "      <th>claimed</th>\n",
       "      <th>do</th>\n",
       "      <th>earlier</th>\n",
       "      <th>election</th>\n",
       "      <th>elections</th>\n",
       "      <th>friend</th>\n",
       "      <th>friends</th>\n",
       "      <th>had</th>\n",
       "      <th>he</th>\n",
       "      <th>his</th>\n",
       "      <th>in</th>\n",
       "      <th>interference</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>lost</th>\n",
       "      <th>minister</th>\n",
       "      <th>mr</th>\n",
       "      <th>no</th>\n",
       "      <th>nothing</th>\n",
       "      <th>of</th>\n",
       "      <th>outcome</th>\n",
       "      <th>parties</th>\n",
       "      <th>political</th>\n",
       "      <th>post</th>\n",
       "      <th>president</th>\n",
       "      <th>prime</th>\n",
       "      <th>putin</th>\n",
       "      <th>republican</th>\n",
       "      <th>russia</th>\n",
       "      <th>says</th>\n",
       "      <th>served</th>\n",
       "      <th>some</th>\n",
       "      <th>support</th>\n",
       "      <th>the</th>\n",
       "      <th>though</th>\n",
       "      <th>to</th>\n",
       "      <th>trump</th>\n",
       "      <th>vladimir</th>\n",
       "      <th>was</th>\n",
       "      <th>who</th>\n",
       "      <th>winning</th>\n",
       "      <th>witchhunt</th>\n",
       "      <th>with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_trump</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_election</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_putin</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              after  as  became  by  career  ...  was  who  winning  witchhunt  with\n",
       "doc_trump         1   0       1   0       0  ...    0    0        1          0     1\n",
       "doc_election      0   0       0   1       0  ...    1    1        0          1     1\n",
       "doc_putin         0   1       1   0       1  ...    0    0        0          0     0\n",
       "\n",
       "[3 rows x 48 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scikit Learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Create the Document Term Matrix\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_vectorizer = CountVectorizer()\n",
    "sparse_matrix = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "# OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "doc_term_matrix = sparse_matrix.todense()\n",
    "df = pd.DataFrame(doc_term_matrix, \n",
    "                  columns=count_vectorizer.get_feature_names(), \n",
    "                  index=['doc_trump', 'doc_election', 'doc_putin'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "tB1aXPJEEgn9",
    "outputId": "454c0188-2837-4ed7-dab6-402f3dd9a4e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.51480485 0.38890873]\n",
      " [0.51480485 1.         0.38829014]\n",
      " [0.38890873 0.38829014 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Compute Cosine Similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(df, df))\n",
    "#> [[ 1.          0.48927489  0.37139068]\n",
    "#>  [ 0.48927489  1.          0.38829014]\n",
    "#>  [ 0.37139068  0.38829014  1.        ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XNTDH7aJfDht"
   },
   "source": [
    "## Cosine Similarity Ctd. - Soft Cosine Similarity Analysis ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "clm2AUrfEtGP"
   },
   "source": [
    "We have found the cosine similarity of the captions, as shown above. \n",
    "\n",
    "A soft cosine or (\"soft\" similarity) between two vectors considers similarities between pairs of features, which helps generalise the idea of cosine similiarity. \n",
    "\n",
    "We will take a step further and do a soft cosine similarity analysis with the same list of lists of nouns, and plot a visual representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t0vMhlbpEsfX"
   },
   "outputs": [],
   "source": [
    "# Define the documents\n",
    "doc_soup = \"Soup is a primarily liquid food, generally served warm or hot (but may be cool or cold), \\\n",
    "that is made by combining ingredients of meat or vegetables with stock, juice, water, or another liquid. \"\n",
    "\n",
    "doc_noodles = \"Noodles are a staple food in many cultures. They are made from unleavened dough which is \\\n",
    "stretched, extruded, or rolled flat and cut into one of a variety of shapes.\"\n",
    "\n",
    "doc_dosa = \"Dosa is a type of pancake from the Indian subcontinent, made from a fermented batter. \\\n",
    "It is somewhat similar to a crepe in appearance. Its main ingredients are rice and black gram.\"\n",
    "\n",
    "documents = [doc_trump, doc_election, doc_putin, doc_soup, doc_noodles, doc_dosa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "Lh7YQL9UEwFE",
    "outputId": "b776fb4f-d9d7-4c6e-a451-115f3d82fa84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.0\n",
      "[================================================--] 96.2% 922.1/958.4MB downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "# upgrade gensim if you can't import softcossim\n",
    "from gensim.matutils import softcossim \n",
    "from gensim import corpora\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess\n",
    "print(gensim.__version__)\n",
    "#> '3.6.0'\n",
    "\n",
    "# Download the FastText model\n",
    "fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "_UyGJvuQExWA",
    "outputId": "258e0bf7-c498-4caf-864b-f0a5a6d7be82"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5842470477718544\n"
     ]
    }
   ],
   "source": [
    "# Prepare a dictionary and a corpus.\n",
    "dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in documents])\n",
    "\n",
    "# Prepare the similarity matrix\n",
    "similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
    "\n",
    "# Convert the sentences into bag-of-words vectors.\n",
    "sent_1 = dictionary.doc2bow(simple_preprocess(doc_trump))\n",
    "sent_2 = dictionary.doc2bow(simple_preprocess(doc_election))\n",
    "sent_3 = dictionary.doc2bow(simple_preprocess(doc_putin))\n",
    "sent_4 = dictionary.doc2bow(simple_preprocess(doc_soup))\n",
    "sent_5 = dictionary.doc2bow(simple_preprocess(doc_noodles))\n",
    "sent_6 = dictionary.doc2bow(simple_preprocess(doc_dosa))\n",
    "\n",
    "sentences = [sent_1, sent_2, sent_3, sent_4, sent_5, sent_6]\n",
    "\n",
    "# Compute soft cosine similarity\n",
    "print(softcossim(sent_1, sent_2, similarity_matrix))\n",
    "#> 0.567228632589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "47w2GT-OE2O7",
    "outputId": "afa06b5c-2af0-4d09-ced6-35b149c6f927"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.58</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5\n",
       "0  1.00  0.58  0.56  0.28  0.34  0.40\n",
       "1  0.58  1.00  0.54  0.25  0.31  0.43\n",
       "2  0.56  0.54  1.00  0.19  0.25  0.36\n",
       "3  0.28  0.25  0.19  1.00  0.50  0.38\n",
       "4  0.34  0.31  0.25  0.50  1.00  0.56\n",
       "5  0.40  0.43  0.36  0.38  0.56  1.00"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_soft_cossim_matrix(sentences):\n",
    "    len_array = np.arange(len(sentences))\n",
    "    xx, yy = np.meshgrid(len_array, len_array)\n",
    "    cossim_mat = pd.DataFrame([[round(softcossim(sentences[i],sentences[j], similarity_matrix) ,2) for i, j in zip(x,y)] for y, x in zip(xx, yy)])\n",
    "    return cossim_mat\n",
    "\n",
    "create_soft_cossim_matrix(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2FNxNNqtftc3"
   },
   "source": [
    "# Conclusion #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2lYK52d6gbI7"
   },
   "source": [
    "*Matthew do this part*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "403 final.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
